[Unit]
Description=RHOIM vLLM
After=network-online.target rhoim-model-pull.service
# Optional GPU guard:
# ExecStartPre=/usr/bin/nvidia-smi

[Service]
EnvironmentFile=-/etc/sysconfig/rhoim
ExecStart=/opt/rhoim/venv/bin/python3 -m vllm.entrypoints.openai.api_server \
  --model ${MODEL_URI} --host 0.0.0.0 --port 8000 \
  --device cpu --dtype float32 --attention-backend torch \
  --max-model-len 8192 --enforce-eager --disable-log-requests
Restart=always

[Install]
WantedBy=multi-user.target
