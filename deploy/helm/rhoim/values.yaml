image:
  gateway: quay.io/you/rhoim-gateway:latest
  vllm: quay.io/you/rhoim-vllm:latest
  appliance: quay.io/you/rhoim-appliance:latest

singleContainer: false  # set true to deploy appliance mode on K8s/OCP

resources:
  gateway:
    requests: {cpu: "250m", memory: "512Mi"}
    limits:   {cpu: "1",    memory: "1Gi"}
  vllm:
    requests: {cpu: "2", memory: "20Gi", nvidia.com/gpu: 1}
    limits:   {cpu: "4", memory: "24Gi", nvidia.com/gpu: 1}
  appliance:
    requests: {cpu: "2", memory: "20Gi", nvidia.com/gpu: 1}
    limits:   {cpu: "4", memory: "24Gi", nvidia.com/gpu: 1}

gpu:
  count: 1

model:
  name: llama3-8b-instruct
  uri: /models/llama3-8b
  source: ""   # hf://meta-llama/Meta-Llama-3-8B-Instruct or s3://bucket/path

service:
  type: ClusterIP
  port: 8080   # gateway/appliance external port

route:
  enabled: true
  host: ""

metrics:
  enabled: true

basePath: "/"
enableRhoaiAlias: true
